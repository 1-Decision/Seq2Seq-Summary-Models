{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47817992-117b-4a7a-9489-d26ffb62db32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-13T16:49:52.800006Z",
     "iopub.status.busy": "2022-08-13T16:49:52.799583Z",
     "iopub.status.idle": "2022-08-13T16:49:53.349699Z",
     "shell.execute_reply": "2022-08-13T16:49:53.348890Z",
     "shell.execute_reply.started": "2022-08-13T16:49:52.799976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7133d7ff-3af1-4789-979c-71c09e731c4b",
   "metadata": {},
   "source": [
    "# Do not run subsequent cell unless gptj.pt doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeaed773-83e1-46cf-a2a9-5befe97e4b9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-13T19:41:04.270404Z",
     "iopub.status.busy": "2022-08-13T19:41:04.269856Z",
     "iopub.status.idle": "2022-08-13T19:41:04.273149Z",
     "shell.execute_reply": "2022-08-13T19:41:04.272597Z",
     "shell.execute_reply.started": "2022-08-13T19:41:04.270379Z"
    }
   },
   "outputs": [],
   "source": [
    "#from transformers import GPTJForCausalLM, AutoTokenizer\n",
    "#import torch    \n",
    "\n",
    "#model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\") #, revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "#torch.save(model, \"gptj.pt\")\n",
    "#torch.save(tokenizer, \"tokenizer.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa7565a-d783-45e8-95ad-b37062940906",
   "metadata": {},
   "source": [
    "# Load Presaved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a6ed796-24fe-40ec-954f-a19cc5d820fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-13T19:45:18.225579Z",
     "iopub.status.busy": "2022-08-13T19:45:18.225307Z",
     "iopub.status.idle": "2022-08-13T19:45:49.836296Z",
     "shell.execute_reply": "2022-08-13T19:45:49.835615Z",
     "shell.execute_reply.started": "2022-08-13T19:45:18.225560Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPTJForCausalLM\n",
    "import torch\n",
    "\n",
    "model = torch.load(\"gptj.pt\")\n",
    "\n",
    "tokenizer = torch.load(\"tokenizer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1db9e40a-4e61-4824-8b72-5215afce16d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-13T19:48:08.161730Z",
     "iopub.status.busy": "2022-08-13T19:48:08.161462Z",
     "iopub.status.idle": "2022-08-13T19:48:08.907384Z",
     "shell.execute_reply": "2022-08-13T19:48:08.906723Z",
     "shell.execute_reply.started": "2022-08-13T19:48:08.161711Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'GPTJForCausalLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'PegasusForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'T5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Your max_length is set to 50, but you input_length is only 35. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'https://forefrontai.medium.com/summarize-articles-documents-books-with-gpt-j-64fdd7dd935a\\n======\\nneiman-smith\\nA different style of summarization'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# create pipeline\n",
    "gen = pipeline(\"summarization\",model=model,tokenizer=tokenizer,device=0)\n",
    "\n",
    "# run prediction\n",
    "gen(\"https://forefrontai.medium.com/summarize-articles-documents-books-with-gpt-j-64fdd7dd935\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f746ba-419c-46f6-bf35-bc51cff84374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
